---
---

@article{LONNFALT2025112202,
abbr={JSS},
title = {An intelligent test management system for optimizing decision making during software testing},
journal = {Journal of Systems and Software},
volume = {219},
pages = {112202},
year = {2025},
issn = {0164-1212},
doi = {https://doi.org/10.1016/j.jss.2024.112202},
url = {https://www.sciencedirect.com/science/article/pii/S0164121224002462},
author = {Albin Lönnfält and Viktor Tu and Gregory Gay and Animesh Singh and Sahar Tahvili},
keywords = {Software testing, Decision support, Continuous integration, Natural language processing, Machine learning},
abstract = {To ensure the proper testing of any software product, it is imperative to cover various functional and non-functional requirements at different testing levels (e.g., unit or integration testing). Ensuring appropriate testing requires making a series of decisions—e.g., assigning features to distinct Continuous Integration (CI) configurations or determining which test specifications to automate. Such decisions are generally made manually and require in-depth domain knowledge. This study introduces, implements, and evaluates ITMOS (Intelligent Test Management Optimization System), an intelligent test management system designed to optimize decision-making during the software testing process. ITMOS efficiently processes new requirements presented in natural language, segregating each requirement into appropriate CI configurations based on predefined quality criteria. Additionally, ITMOS has the capability to suggest a set of test specifications for test automation. The feasibility and potential applicability of the proposed solution were empirically evaluated in an industrial telecommunications project at Ericsson. In this context, ITMOS achieved accurate results for decision-making tasks, exceeding the requirements set by domain experts.},
pdf={https://greg4cr.github.io/pdf/24CIConfigPrediction.pdf},
}

@misc{liu2025exploringintegrationlargelanguage,
      abbr={Pre-Print},
      title={Exploring the Integration of Large Language Models in Industrial Test Maintenance Processes}, 
      author={Jingxiong Liu and Ludvig Lemner and Linnea Wahlgren and Gregory Gay and Nasser Mohammadiha and Joakim Wennerberg},
      year={2025},
      eprint={2409.06416},
      archivePrefix={arXiv},
      primaryClass={cs.SE},
      url={https://arxiv.org/abs/2409.06416}, 
      abstract={Much of the cost and effort required during the software testing process is invested in performing test maintenance - the addition, removal, or modification of test cases to keep the test suite in sync with the system-under-test or to otherwise improve its quality. Tool support could reduce the cost - and improve the quality - of test maintenance by automating aspects of the process or by providing guidance and support to developers.
In this study, we explore the capabilities and applications of large language models (LLMs) - complex machine learning models adapted to textual analysis - to support test maintenance. We conducted a case study at Ericsson AB where we explore the triggers that indicate the need for test maintenance, the actions that LLMs can take, and the considerations that must be made when deploying LLMs in an industrial setting. We also propose and demonstrate a multi-agent architecture that can predict which tests require maintenance following a change to the source code. Collectively, these contributions advance our theoretical and practical understanding of how LLMs can be deployed to benefit industrial test maintenance processes.},
      pdf={https://arxiv.org/pdf/2409.06416},
      html={https://arxiv.org/html/2409.06416v2},
}

@inproceedings{10.1145/3691620.3695273,
abbr={ASE},
author = {van Heijningen, Stefan Alexander and Wiik, Theo and Neto, Francisco Gomes de Oliveira and Gay, Gregory and Viggedal, Kim and Friberg, David},
title = {Integrating Mutation Testing Into Developer Workflow: An Industrial Case Study},
year = {2024},
isbn = {9798400712487},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3691620.3695273},
doi = {10.1145/3691620.3695273},
abstract = {Mutation testing is a potentially effective method to assess test suite adequacy. Researchers have made mutation testing more computationally efficient, and new frameworks are regularly emerging. However, there is still limited adoption of mutation testing in industry. We hypothesize that such adoption is hindered by a lack of guidance on how to effectively and efficiently utilize mutation testing in a development workflow. To that end, we have conducted an industrial case study exploring the technical challenges of implementing mutation testing in continuous integration, what information from mutation testing is of use to developers, and how that information should be presented (in textual and visual form). Our results reveal five technical challenges of integrating mutation testing and nine key findings regarding how the results of mutation testing are used and presented. We also offer a dashboard to visualize mutation testing results, as well as 16 recommendations for making effective use of mutation testing in practice1.},
booktitle = {Proceedings of the 39th IEEE/ACM International Conference on Automated Software Engineering},
pages = {2110–2120},
numpages = {11},
keywords = {mutation testing, test adequacy, software visualization, software testing},
location = {Sacramento, CA, USA},
series = {ASE '24},
pdf={https://greg4cr.github.io/pdf/24mutation.pdf},
}
